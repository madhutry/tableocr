{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Untitled1.ipynb","provenance":[],"mount_file_id":"1q83KZYRGkM0Yrnk2-Dtyb-aaVKrXoEVk","authorship_tag":"ABX9TyPDsnXDH3sWjvR0QA+lCdHe"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"t1xAt5866W8A","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":124},"executionInfo":{"status":"ok","timestamp":1600008346000,"user_tz":-330,"elapsed":65933,"user":{"displayName":"Madhukar Tryambake","photoUrl":"","userId":"15743648273766689179"}},"outputId":"b92b9d70-9199-4d5c-a81c-5f9b13e5d2d3"},"source":["#@title Install and copy sample { form-width: \"10%\" }\n","\n","# install dependencies: \n","!pip install pyyaml==5.1 pycocotools>=2.0.1\n","!pip -q install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu101/torch1.6/index.html\n","!pip install shapely\n","!unzip -q \"/content/drive/My Drive/final_projects/icdar.zip\"\n","!unzip -q '/content/drive/My Drive/final_projects/bankdata1.zip'\n","!ln -s '/content/drive/My Drive/work_dirs/' /content/workdir\n","!ln -s '/content/drive/My Drive/detectron2/' /content/detectron2\n","!ln -s '/content/drive/My Drive/final_projects/detection/' /content/detection"],"execution_count":1,"outputs":[{"output_type":"stream","text":["\u001b[K     |████████████████████████████████| 6.6MB 555kB/s \n","\u001b[K     |████████████████████████████████| 2.2MB 4.5MB/s \n","\u001b[?25h  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n","Requirement already satisfied: shapely in /usr/local/lib/python3.6/dist-packages (1.7.1)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"CX8LRGyC6UXd","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600008811005,"user_tz":-330,"elapsed":1587,"user":{"displayName":"Madhukar Tryambake","photoUrl":"","userId":"15743648273766689179"}}},"source":["#@title IMport { form-width: \"10%\" }\n","\n","# Some basic setup:\n","# Setup detectron2 logger\n","import detectron2\n","from detectron2.utils.logger import setup_logger\n","setup_logger()\n","\n","# import some common libraries\n","import numpy as np\n","import os, json, cv2, random\n","from google.colab.patches import cv2_imshow\n","\n","# import some common detectron2 utilities\n","from detectron2 import model_zoo\n","from detectron2.engine import DefaultPredictor\n","from detectron2.config import get_cfg\n","from detectron2.utils.visualizer import Visualizer\n","from detectron2.data import MetadataCatalog, DatasetCatalog\n","import matplotlib.pyplot  as plt"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"Lm9_ecFX8VC6","colab_type":"code","colab":{}},"source":["#@title Verify Single { form-width: \"10%\" }\n","from detectron2.structures import BitMasks, Boxes, BoxMode, Keypoints, PolygonMasks, RotatedBoxes\n","import glob\n","cfg = get_cfg()\n","cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"))\n","cfg.DATALOADER.NUM_WORKERS = 2\n","cfg.SOLVER.IMS_PER_BATCH = 2\n","cfg.SOLVER.BASE_LR = 0.00025  # pick a good LR\n","cfg.SOLVER.MAX_ITER = 300    # 300 iterations seems good enough for this toy dataset; you may need to train longer for a practical dataset\n","cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128   # faster, and good enough for this toy dataset (default: 512)\n","cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1  # only has one class (ballon)\n","cfg.MODEL.WEIGHTS = '/content/detection/savedmodel/model_final.pth'\n","cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.8   # set the testing threshold for this model\n","files = glob.glob('/content/detectron2/TestImages/*.jpg')\n","for fno,f in enumerate(files):\n","  im = cv2.imread(f)\n","  predictor = DefaultPredictor(cfg)\n","  print('Predicting',f)\n","  outputs = predictor(im)\n","  print('Predicting Done')\n","  MetadataCatalog.get(\"table_bank\").set(thing_classes=[\"table\"])\n","  table_metadata = MetadataCatalog.get(\"table_bank\")\n","  # v = Visualizer(im[:, :, ::-1],\n","  #                 metadata=table_metadata, \n","  #                 scale=0.8\n","  # )\n","  # out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n","  # cv2_imshow(out.get_image()[:, :, ::-1])\n","\n","  predictions = outputs[\"instances\"].to(\"cpu\")\n","  boxesTn = predictions.pred_boxes.tensor.numpy() if predictions.has(\"pred_boxes\") else None\n","  # scores = predictions.scores if predictions.has(\"scores\") else None\n","  # classes = predictions.pred_classes.numpy() if predictions.has(\"pred_classes\") else None\n","  if isinstance(boxesTn, Boxes) or isinstance(boxesTn, RotatedBoxes):\n","    boxes = boxesTn.tensor.numpy()\n","  else:\n","    boxes=np.asarray(boxesTn)\n","  for boxno in range(len(boxes)):\n","    x1,y1,x2,y3 = int(boxes[boxno][0]),int(boxes[boxno][1]),int(boxes[boxno][2]),int(boxes[boxno][3])\n","    crop = im[y1:y3,x1:x2]\n","    cv2.imwrite('/content/box_'+str(fno)+'_'+str(boxno)+'.jpg',crop)\n","    plt.imshow(crop)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TrOTttrH8sL7","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":376},"executionInfo":{"status":"error","timestamp":1600010785658,"user_tz":-330,"elapsed":1375,"user":{"displayName":"Madhukar Tryambake","photoUrl":"","userId":"15743648273766689179"}},"outputId":"e91b9ae0-8220-46a9-a347-8e187072cce0"},"source":["#@title CRAFT IMPORT LOAD  MODEL{ form-width: \"10%\" }\n","import sys\n","import os\n","import time\n","import argparse\n","\n","import torch\n","import torch.nn as nn\n","import torch.backends.cudnn as cudnn\n","from torch.autograd import Variable\n","\n","from PIL import Image\n","\n","import cv2\n","from skimage import io\n","import numpy as np\n","import craft_utils\n","import imgproc\n","import file_utils\n","import json\n","import zipfile\n","from craft import CRAFT\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from collections import OrderedDict\n","import h5py\n","import croputil \n","import importlib\n","importlib.reload(croputil)\n","from croputil import *\n","\n","\n","train_model='/content/workdir/craft_ic15_20k.pth'\n","\n","class args :\n","  trained_model = train_model\n","  cuda=True\n","  text_threshold=0.7\n","  low_text=0.4\n","  link_threshold=0.4\n","  canvas_size=1280\n","  mag_ratio=1.5\n","  poly=False\n","  show_time=False\n","  test_folder='/data/'\n","  refine=False\n","  refiner_model='weights/craft_refiner_CTW1500.pth'\n","\n","\n","def copyStateDict(state_dict):\n","    if list(state_dict.keys())[0].startswith(\"module\"):\n","        start_idx = 1\n","    else:\n","        start_idx = 0\n","    new_state_dict = OrderedDict()\n","    for k, v in state_dict.items():\n","        name = \".\".join(k.split(\".\")[start_idx:])\n","        new_state_dict[name] = v\n","    return new_state_dict\n","\n","def str2bool(v):\n","    return v.lower() in (\"yes\", \"y\", \"true\", \"t\", \"1\")\n","\n","def test_net(net, image, text_threshold, link_threshold, low_text, cuda, poly, refine_net=None):\n","    t0 = time.time()\n","\n","    # resize\n","    img_resized, target_ratio, size_heatmap = imgproc.resize_aspect_ratio(image, args.canvas_size, interpolation=cv2.INTER_LINEAR, mag_ratio=args.mag_ratio)\n","    ratio_h = ratio_w = 1 / target_ratio\n","\n","    # preprocessing\n","    x = imgproc.normalizeMeanVariance(img_resized)\n","    x = torch.from_numpy(x).permute(2, 0, 1)    # [h, w, c] to [c, h, w]\n","    x = Variable(x.unsqueeze(0))                # [c, h, w] to [b, c, h, w]\n","    if cuda:\n","        x = x.cuda()\n","\n","    # forward pass\n","    with torch.no_grad():\n","        y, feature = net(x)\n","\n","    # make score and link map\n","    score_text = y[0,:,:,0].cpu().data.numpy()\n","    score_link = y[0,:,:,1].cpu().data.numpy()\n","\n","    # refine link\n","    if refine_net is not None:\n","        with torch.no_grad():\n","            y_refiner = refine_net(y, feature)\n","        score_link = y_refiner[0,:,:,0].cpu().data.numpy()\n","\n","    t0 = time.time() - t0\n","    t1 = time.time()\n","\n","    # Post-processing\n","    boxes, polys = craft_utils.getDetBoxes(score_text, score_link, text_threshold, link_threshold, low_text, poly)\n","\n","    # coordinate adjustment\n","    boxes = craft_utils.adjustResultCoordinates(boxes, ratio_w, ratio_h)\n","    polys = craft_utils.adjustResultCoordinates(polys, ratio_w, ratio_h)\n","    for k in range(len(polys)):\n","        if polys[k] is None: polys[k] = boxes[k]\n","\n","    t1 = time.time() - t1\n","\n","    # render results (optional)\n","    render_img = score_text.copy()\n","    render_img = np.hstack((render_img, score_link))\n","    ret_score_text = imgproc.cvt2HeatmapImg(render_img)\n","\n","    if args.show_time : print(\"\\ninfer/postproc time : {:.3f}/{:.3f}\".format(t0, t1))\n","\n","    return boxes, polys, ret_score_text\n","\n","def loadCraftModel():\n","  net = CRAFT()\n","  print('Loading weights from checkpoint (' + args.trained_model + ')')\n","  if args.cuda:\n","      net.load_state_dict(copyStateDict(torch.load(args.trained_model)))\n","  else:\n","      net.load_state_dict(copyStateDict(torch.load(args.trained_model, map_location='cpu')))\n","  if args.cuda:\n","      net = net.cuda()\n","      net = torch.nn.DataParallel(net)\n","      cudnn.benchmark = False\n","  net.eval()\n","  return net\n","\n","net=loadCraftModel()\n"],"execution_count":14,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-8702889ae3dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mcroputil\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcroputil\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'croputil'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"]}]},{"cell_type":"code","metadata":{"id":"m88tjxx1AnRz","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":104},"executionInfo":{"status":"ok","timestamp":1600010499205,"user_tz":-330,"elapsed":2756,"user":{"displayName":"Madhukar Tryambake","photoUrl":"","userId":"15743648273766689179"}},"outputId":"610ef8ba-86a9-42a8-80a8-81416cf19da8"},"source":["%cd /content\n","!git clone https://github.com/clovaai/CRAFT-pytorch.git"],"execution_count":11,"outputs":[{"output_type":"stream","text":["/content\n","Cloning into 'CRAFT-pytorch'...\n","remote: Enumerating objects: 56, done.\u001b[K\n","remote: Total 56 (delta 0), reused 0 (delta 0), pack-reused 56\u001b[K\n","Unpacking objects: 100% (56/56), done.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"FUmuhbqiDqkn","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1600010735723,"user_tz":-330,"elapsed":1250,"user":{"displayName":"Madhukar Tryambake","photoUrl":"","userId":"15743648273766689179"}},"outputId":"b1a4a229-b9c7-4bdd-cfe3-08056b770490"},"source":["%cd /content/CRAFT-pytorch/\n"],"execution_count":12,"outputs":[{"output_type":"stream","text":["/content/CRAFT-pytorch\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"C7jAtHJrEksI","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":72},"executionInfo":{"status":"ok","timestamp":1600010753846,"user_tz":-330,"elapsed":1665,"user":{"displayName":"Madhukar Tryambake","photoUrl":"","userId":"15743648273766689179"}},"outputId":"12d18282-bf5d-4d27-a7c9-e70b44a75819"},"source":[""],"execution_count":13,"outputs":[{"output_type":"stream","text":["\u001b[31mERROR: Could not find a version that satisfies the requirement torch==0.4.1.post2 (from -r requirements.txt (line 1)) (from versions: 0.1.2, 0.1.2.post1, 0.1.2.post2, 0.3.1, 0.4.0, 0.4.1, 1.0.0, 1.0.1, 1.0.1.post2, 1.1.0, 1.2.0, 1.3.0, 1.3.1, 1.4.0, 1.5.0, 1.5.1, 1.6.0)\u001b[0m\n","\u001b[31mERROR: No matching distribution found for torch==0.4.1.post2 (from -r requirements.txt (line 1))\u001b[0m\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0QILkw2WEpAy","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}